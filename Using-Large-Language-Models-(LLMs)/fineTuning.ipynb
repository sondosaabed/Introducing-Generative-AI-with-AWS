{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data of SEC filing of Amazon has been pre-saved in the S3 bucket.\n",
    "from sagemaker.jumpstart.utils import get_jumpstart_content_bucket\n",
    "\n",
    "from sagemaker import hyperparameters\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "from sagemaker import TrainingJobAnalytics\n",
    "from sagemaker.jumpstart.model import JumpStartModel \n",
    "import json\n",
    "\n",
    "aws_region = ''\n",
    "model_version = '1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idenfity the foundation model to fine-tune\n",
    "model_id = \"huggingface-llm-falcon-7b-bf16\"\n",
    "\n",
    "# Sample training data is available in this bucket\n",
    "data_bucket = get_jumpstart_content_bucket(aws_region)\n",
    "data_prefix = \"training-datasets/sec_data\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/train/\"\n",
    "validation_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/validation/\"\n",
    "\n",
    "#Prepare training parameters\n",
    "my_hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "my_hyperparameters[\"epoch\"] = \"3\"\n",
    "my_hyperparameters[\"per_device_train_batch_size\"] = \"2\"\n",
    "my_hyperparameters[\"instruction_tuned\"] = \"False\"\n",
    "print(my_hyperparameters)\n",
    "\n",
    "#Validate hyperparameters\n",
    "hyperparameters.validate(model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters)\n",
    "\n",
    "# Starting training\n",
    "domain_adaptation_estimator = JumpStartEstimator(model_id=model_id, hyperparameters=my_hyperparameters, instance_type=\"ml.p3dn.24xlarge\",)\n",
    "\n",
    "domain_adaptation_estimator.fit({\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path}, logs=True)\n",
    "#Extract Training performance metrics. Performance metrics such as training loss and validation accuracy/loss can be accessed through cloudwatch while the training. We can also fetch these metrics and analyze them within the notebook\n",
    "\n",
    "training_job_name = domain_adaptation_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying inference endpoints\n",
    "# We deploy the domain-adaptation fine-tuned and pretrained models separately, and compare their performances.\n",
    "# We first deploy the domain-adaptation fine-tuned model.\n",
    "domain_adaptation_predictor = domain_adaptation_estimator.deploy()\n",
    "#Next, we deploy the pre-trained huggingface-llm-falcon-7b-bf16.\n",
    "my_model = JumpStartModel(model_id=model_id)\n",
    "pretrained_predictor = my_model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running inference queries and compare model performances\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(endpoint_name, text):\n",
    "    payload = {\"inputs\": f\"{text}:\", \"parameters\": parameters}\n",
    "    query_response = query_endpoint_with_json_payload(json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name)\n",
    "    generated_texts = parse_response(query_response)\n",
    "    print(f\"Response: {generated_texts}{\"\\n\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paragraph_domain_adaption = [\n",
    "    \"This Form 10-K report shows that\",\n",
    "    \"We serve consumers through\",\n",
    "    \"Our vision is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in test_paragraph_domain_adaption:\n",
    "    print(\"-\" * 80)\n",
    "    print(paragraph)\n",
    "    print(\"-\" * 80)\n",
    "    print(\"pre-trained\")\n",
    "    generate_response(pretrained_predictor.endpoint_name, paragraph)\n",
    "    print(\"fine-tuned\")\n",
    "    generate_response(domain_adaptation_predictor.endpoint_name, paragraph)\n",
    "\n",
    "# The fine-tuned model starts to generate responses that are more specific to the domain of fine-tuning data which is relating to SEC report of Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the endpoint\n",
    "# Delete the SageMaker endpoint\n",
    "pretrained_predictor.delete_model()\n",
    "pretrained_predictor.delete_endpoint()\n",
    "domain_adaptation_predictor.delete_model()\n",
    "domain_adaptation_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
